{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb887d5",
   "metadata": {},
   "source": [
    "# Lab 4: Class Imbalance and Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3dde8",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "A little classification problem on glass type recognition.\n",
    "\n",
    "1. Import the `glass.dat` dataset;\n",
    "2. print the data  and the classes (column `type`);\n",
    "3. divide the dataset into training and test set with the 20% of the whole data for testing;\n",
    "4. print the number of training and test instances;\n",
    "5. train a SVM and compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e7331",
   "metadata": {},
   "source": [
    "Are we happy about this accuracy? Let's look the f1-score = harmonic mean of the precision and recall with `metrics.f1_score`\n",
    "\n",
    "$$\n",
    "    f1 = 2 \\cdot \\dfrac{(precision \\cdot recall)}{(precision + recall)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738accad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.recall_score(y_pred,y_test)) # recall = tp / (tp + fn) \n",
    "print(metrics.precision_score(y_pred,y_test)) # precision =  tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11bcc9",
   "metadata": {},
   "source": [
    "Both precision and recall are 0, but why? Let's print the entries of the confusion matrix with `metrics.confusion_matrix`.\n",
    "\n",
    "\n",
    "<img src=\"img/conf_mat.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92026f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"confusion matrix \\n\",conf_mat,\"\\n\")\n",
    "print(\"True negative    \\t\",conf_mat[0][0],\"--> predicted as negative, and really negative :) \")\n",
    "print(\"False positive   \\t\",conf_mat[0][1],\" --> predicted as positive, but negative :( \")\n",
    "print(\"False negative  \\t\",conf_mat[1][0],\" --> predicted as negative, but positive :( \")\n",
    "print(\"True positive   \\t\",conf_mat[1][1],\" --> predicted as positive, and really positive :) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee63dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the predictions:\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54c5a6",
   "metadata": {},
   "source": [
    "The classifier predicts only one classe (the class zero), that's really bad! Any idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3397f4",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "1. Print the number of instances of class 0 and 1. Hint: you can use `np.unique` by setting a proper input parameter, have a look at the documentation.\n",
    "2. Print the ratio of positive samples over the total samples -> the skewness.\n",
    "3. Print the ratio of negative samples over the total samples.\n",
    "4. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065d830",
   "metadata": {},
   "source": [
    "It is a good practice to compute the positive and negative ratio before training a ML to understand the imbalance of the data.\n",
    "\n",
    "In the next exercise we address the imbalance by oversampling the minority class: giving more weights to the minority class corresponds to duplicate the samples of the minority class to reach the same number of the majority class.\n",
    "\n",
    "**Exercise 3**\n",
    "1. Train a SVM by including the class weights (see the doc, use both the `class_weight` option);\n",
    "2. The `class_weight` option takes a dict as `{0: weight_class_0, 1: weight_class_1}`, we can give weight 1 to the most popular class and a bigger weight to the minority class.\n",
    "3. Print precision, recall, f1 score, accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe62f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035dbc1",
   "metadata": {},
   "source": [
    "## Plot Receiver Operating Characteristic and Precision/Recall curves\n",
    "\n",
    "Sklearn provides you utilities for computing the ROC and the PrecRec curve: the `roc_curve` and `precision_recall_curve` functions in `sklearn.metrics`. In addition, the `auc` function returns the area under each curve.\n",
    "\n",
    "**Exercise 4**\n",
    "Plot the ROC curve for each class of the Iris dataset. Understand and complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1511e0-c4f7-4e3a-8224-ace930f573b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "# Import the IRIS dataset by using the datasets.load_iris() function and inspect its attributes\n",
    "# YOUR CODE HERE\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(10)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets: test size 50%, random_state=0\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel=\"linear\", probability=True, random_state=random_state))\n",
    "\n",
    "# Train the classifier and compute the scores of the test set (look at the SVM methods)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "# Understand what the roc_curve() does and takes as input\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "th = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for each class with the AUC in the legend. Plot also the line from (0, 0) to (1, 1)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c67e9d-359d-483a-82f8-41b75913a9c1",
   "metadata": {},
   "source": [
    "This task can be performed with the `RocCurveDisplay` function in `sklearn.metrics`. The boolean parameter `plot_chance_level` (new from version 1.3.) determines whether to plot the baseline level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78379afd-2b5f-41d7-9618-0493be8d2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_test[:, class_id],\n",
    "        y_score[:, class_id],\n",
    "        name=f\"ROC curve for {class_name}\",\n",
    "        color=colors[class_id],\n",
    "        ax=ax)\n",
    "    \n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a448f26",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Repeat the same exercise by plotting the Precision Recall curve (search for the `precision_recall_curve` SKlearn function) with the baseline: P/(P + N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309f2e1-0bb5-47c0-877c-58a39a69e7fb",
   "metadata": {},
   "source": [
    "This task can be performed with the `PrecisionRecallDisplay` function in `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7618d6-393a-4858-aba6-549a2551e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_title=\"Precision Recall Curve\\nto One-vs-Rest multiclass\",\n",
    "\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    precision[class_id], recall[class_id], _ = precision_recall_curve(y_test[:, class_id], y_score[:, class_id])\n",
    "    average_precision[class_id] = average_precision_score(y_test[:, class_id], y_score[:, class_id])\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[class_id],\n",
    "        precision=precision[class_id],\n",
    "        average_precision=average_precision[class_id],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"Precision-recall for class {class_name}\", color=colors[class_id])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78fc89",
   "metadata": {},
   "source": [
    "## Model Selection: K fold, cross validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test`, `y_test`. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.\n",
    "\n",
    "One problem with validation sets is that you \"lose\" some of the data: you only used, e.g., 3/4 of the data for the training, and used 1/4 for the validation. One option is to use K-fold cross-validation, where we split the data into chunks and perform fits, where each chunk gets a turn as the validation set. \n",
    "\n",
    "First, we need to understand how to split data into folds by using `KFold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb33148b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold id 0 TRAIN: [  0   1   2   3   6   9  10  11  14  17  19  20  21  23  25  27  28  29\n",
      "  31  32  34  35  36  38  39  41  42  43  46  47  48  49  50  51  53  54\n",
      "  57  58  59  62  64  65  67  68  69  70  72  73  77  78  79  81  82  84\n",
      "  85  87  88  91  92  93  94  95  98  99 100 101 102 103 104 105 107 110\n",
      " 113 114 115 116 117 118 119 120 121 123 124 125 127 128 130 131 132 133\n",
      " 134 140 142 146 147 148 149 150 151 152 156 161 163 164 165 166 167 169\n",
      " 171 172 173 174 175 177 178 179 181 182 183 184 185 186 187 188 190 192\n",
      " 193 195 198 199 200 201 203 204 205 206 207 209 210 211 212 213]\n",
      "Fold id 0 TEST: [  4   5   7   8  12  13  15  16  18  22  24  26  30  33  37  40  44  45\n",
      "  52  55  56  60  61  63  66  71  74  75  76  80  83  86  89  90  96  97\n",
      " 106 108 109 111 112 122 126 129 135 136 137 138 139 141 143 144 145 153\n",
      " 154 155 157 158 159 160 162 168 170 176 180 189 191 194 196 197 202 208]\n",
      "Label counts: (array([0, 1]), array([131,  11]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold id 1 TRAIN: [  1   4   5   7   8   9  11  12  13  15  16  17  18  21  22  24  25  26\n",
      "  28  29  30  31  32  33  34  35  36  37  38  39  40  42  44  45  47  52\n",
      "  53  55  56  57  58  60  61  63  65  66  67  70  71  72  74  75  76  77\n",
      "  79  80  81  82  83  86  87  88  89  90  91  96  97  99 102 103 105 106\n",
      " 108 109 111 112 114 115 117 119 120 122 126 127 128 129 132 133 135 136\n",
      " 137 138 139 140 141 142 143 144 145 147 148 151 153 154 155 157 158 159\n",
      " 160 162 163 164 165 167 168 170 171 172 174 175 176 177 179 180 181 183\n",
      " 184 185 188 189 191 192 193 194 195 196 197 201 202 203 206 208 210]\n",
      "Fold id 1 TEST: [  0   2   3   6  10  14  19  20  23  27  41  43  46  48  49  50  51  54\n",
      "  59  62  64  68  69  73  78  84  85  92  93  94  95  98 100 101 104 107\n",
      " 110 113 116 118 121 123 124 125 130 131 134 146 149 150 152 156 161 166\n",
      " 169 173 178 182 186 187 190 198 199 200 204 205 207 209 211 212 213]\n",
      "Label counts: (array([0, 1]), array([133,  10]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold id 2 TRAIN: [  0   2   3   4   5   6   7   8  10  12  13  14  15  16  18  19  20  22\n",
      "  23  24  26  27  30  33  37  40  41  43  44  45  46  48  49  50  51  52\n",
      "  54  55  56  59  60  61  62  63  64  66  68  69  71  73  74  75  76  78\n",
      "  80  83  84  85  86  89  90  92  93  94  95  96  97  98 100 101 104 106\n",
      " 107 108 109 110 111 112 113 116 118 121 122 123 124 125 126 129 130 131\n",
      " 134 135 136 137 138 139 141 143 144 145 146 149 150 152 153 154 155 156\n",
      " 157 158 159 160 161 162 166 168 169 170 173 176 178 180 182 186 187 189\n",
      " 190 191 194 196 197 198 199 200 202 204 205 207 208 209 211 212 213]\n",
      "Fold id 2 TEST: [  1   9  11  17  21  25  28  29  31  32  34  35  36  38  39  42  47  53\n",
      "  57  58  65  67  70  72  77  79  81  82  87  88  91  99 102 103 105 114\n",
      " 115 117 119 120 127 128 132 133 140 142 147 148 151 163 164 165 167 171\n",
      " 172 174 175 177 179 181 183 184 185 188 192 193 195 201 203 206 210]\n",
      "Label counts: (array([0, 1]), array([138,   5]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"glass.dat\")\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data['type'].values\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold id {fold_id} TRAIN: {train_index}\")\n",
    "    print(f\"Fold id {fold_id} TEST: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Label counts: {np.unique(y_train, return_counts=True)}\")\n",
    "    print(\"--------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784fa9d",
   "metadata": {},
   "source": [
    "We can notice that some y labels are not balanced in the folds, use `StratifiedKFold` to obtain folds with balanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722a79bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold id 0 TRAIN: [  1   2   3   4   5   7   9  11  12  13  14  17  19  20  21  23  25  26\n",
      "  27  28  32  33  35  36  38  39  40  43  44  47  48  49  50  51  52  53\n",
      "  54  57  60  61  62  64  65  67  69  70  71  72  73  74  75  76  79  80\n",
      "  82  83  84  88  89  92  93  94  95  96  97  98  99 100 101 104 105 106\n",
      " 108 110 113 115 117 118 121 122 124 125 127 128 129 131 132 133 134 135\n",
      " 136 137 138 140 141 146 147 149 150 151 154 156 157 160 161 164 165 167\n",
      " 168 170 171 174 175 176 177 178 180 181 182 184 185 186 187 188 189 190\n",
      " 191 193 194 195 196 197 200 201 202 203 207 208 209 210 211 213]\n",
      "Fold id 0 TEST: [  0   6   8  10  15  16  18  22  24  29  30  31  34  37  41  42  45  46\n",
      "  55  56  58  59  63  66  68  77  78  81  85  86  87  90  91 102 103 107\n",
      " 109 111 112 114 116 119 120 123 126 130 139 142 143 144 145 148 152 153\n",
      " 155 158 159 162 163 166 169 172 173 179 183 192 198 199 204 205 206 212]\n",
      "Label counts: (array([0, 1]), array([134,   8]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold id 1 TRAIN: [  0   1   4   5   6   7   8   9  10  12  13  15  16  18  20  21  22  23\n",
      "  24  26  27  29  30  31  32  34  35  37  40  41  42  44  45  46  49  52\n",
      "  54  55  56  57  58  59  62  63  64  66  67  68  70  72  74  77  78  79\n",
      "  80  81  82  83  85  86  87  88  89  90  91  92  94  95  96 100 101 102\n",
      " 103 107 108 109 111 112 114 115 116 117 118 119 120 123 126 128 129 130\n",
      " 134 135 136 138 139 142 143 144 145 147 148 151 152 153 155 156 158 159\n",
      " 161 162 163 166 168 169 170 171 172 173 174 176 178 179 180 181 182 183\n",
      " 188 189 192 193 195 196 197 198 199 200 204 205 206 208 210 212 213]\n",
      "Fold id 1 TEST: [  2   3  11  14  17  19  25  28  33  36  38  39  43  47  48  50  51  53\n",
      "  60  61  65  69  71  73  75  76  84  93  97  98  99 104 105 106 110 113\n",
      " 121 122 124 125 127 131 132 133 137 140 141 146 149 150 154 157 160 164\n",
      " 165 167 175 177 184 185 186 187 190 191 194 201 202 203 207 209 211]\n",
      "Label counts: (array([0, 1]), array([134,   9]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Fold id 2 TRAIN: [  0   2   3   6   8  10  11  14  15  16  17  18  19  22  24  25  28  29\n",
      "  30  31  33  34  36  37  38  39  41  42  43  45  46  47  48  50  51  53\n",
      "  55  56  58  59  60  61  63  65  66  68  69  71  73  75  76  77  78  81\n",
      "  84  85  86  87  90  91  93  97  98  99 102 103 104 105 106 107 109 110\n",
      " 111 112 113 114 116 119 120 121 122 123 124 125 126 127 130 131 132 133\n",
      " 137 139 140 141 142 143 144 145 146 148 149 150 152 153 154 155 157 158\n",
      " 159 160 162 163 164 165 166 167 169 172 173 175 177 179 183 184 185 186\n",
      " 187 190 191 192 194 198 199 201 202 203 204 205 206 207 209 211 212]\n",
      "Fold id 2 TEST: [  1   4   5   7   9  12  13  20  21  23  26  27  32  35  40  44  49  52\n",
      "  54  57  62  64  67  70  72  74  79  80  82  83  88  89  92  94  95  96\n",
      " 100 101 108 115 117 118 128 129 134 135 136 138 147 151 156 161 168 170\n",
      " 171 174 176 178 180 181 182 188 189 193 195 196 197 200 208 210 213]\n",
      "Label counts: (array([0, 1]), array([134,   9]))\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "for fold_id,  (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold id {fold_id} TRAIN: {train_index}\")\n",
    "    print(f\"Fold id {fold_id} TEST: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Label counts: {np.unique(y_train, return_counts=True)}\")\n",
    "    print(\"--------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3230034",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using $K - 1$ of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/grid_search_cross_validation.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "\n",
    "Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques:\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/grid_search_workflow.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ede9e",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "Use the `google_play_store_apps_reviews_training` dataset to perform 5-fold cross-validation:\n",
    "\n",
    "- Look at Lab 3 for the data import, preprocessing and splitting into train and test. Use TfidfVectorizer without stopwords;\n",
    "- Combine a `StratifiedKFold` with with `cross_val_score` of `sklearn.model_selection` to train a Support Vector Machine with linear kernel. As classification score in the `cross_val_score` object use the f1 measure (see [here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)) in the `scoring` parameter (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)). For\n",
    "- print the mean and stdev of the cross validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c75e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4e9f5",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "You can create a Sklearn pipeline to embed several ML steps. Pipelines can be used as normal classifiers, here an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0299bea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6904193624647222 0.03892002616929922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "data = pd.read_csv('google_play_store_apps_reviews_training.csv')\n",
    "\n",
    "# data cleaning\n",
    "data = data.drop('package_name', axis=1)\n",
    "data['review'] = data['review'].str.strip().str.lower().str.replace('[^\\w]', ' ')\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data['review']\n",
    "y = data['polarity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "clf_pipeline = make_pipeline(CountVectorizer(), svm.SVC(kernel='linear', C=1))\n",
    "scores = cross_val_score(clf_pipeline, X_train, y_train, cv=kf, scoring='f1')\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cddca3",
   "metadata": {},
   "source": [
    "Implement the following 3 pipelines for the above classification and evaluate them with cross_val_score:\n",
    "- `TfidfVectorizer()` and `svm.SVC(kernel='linear', C=1)`;\n",
    "- `CountVectorizer()` and the feature `Normalizer()` step, `svm.SVC(kernel='linear', C=1)`;\n",
    "- `CountVectorizer()` and `svm.SVC(kernel='linear', C=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfb48b",
   "metadata": {},
   "source": [
    "**Exercise 8**\n",
    "\n",
    "Repeat exercise 7 by using `GridSearchCV` to perform an exhaustive search over a set of hyperparameters for a SVM with 'rbf' kernel:\n",
    "- user `TfidfVectorizer` as vectorizer;\n",
    "- use `StratifiedKFold` with 5 folds;\n",
    "- you have to define a hyperparameters grid with a Python dict: the keys are the hyperparam name, the values the range of values for that hyperparam;\n",
    "- use `[1, 10, 100]` for C and `[0.01, 0.05, 0.1]` for gamma;\n",
    "- perform `GridSearchCV`: use the f1 for the score and try to run the fold in parallel:look at the documentation how it is used;\n",
    "- export the results in a Pandas dataframe and print it.\n",
    "- What is the hyperparameters combination leading to the best average f1? What is the value of the best average f1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc45b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
