{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb887d5",
   "metadata": {},
   "source": [
    "# Lab 4: Class Imbalance and Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a3dde8",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "A little classification problem on glass type recognition.\n",
    "\n",
    "1. Import the `glass.dat` dataset;\n",
    "2. print the data  and the classes (column `type`);\n",
    "3. divide the dataset into training and test set with the 20% of the whole data for testing;\n",
    "4. print the number of training and test instances;\n",
    "5. train a SVM and compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"glass.dat\")\n",
    "print(f\"Number samples: {len(data)}\")\n",
    "print(f\"Classes: {data['type'].unique()}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20decac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.to_numpy()[:, :-1]\n",
    "y = data.values[:, -1]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa20143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"len X_train\\t{len(X_train)}\")\n",
    "print(f\"len X_test \\t{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a490c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate the model specifying the kernel\n",
    "clf = SVC()\n",
    "\n",
    "# Training\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90e7331",
   "metadata": {},
   "source": [
    "Are we happy about this accuracy? Let's look the f1-score = harmonic mean of the precision and recall with `metrics.f1_score`\n",
    "\n",
    "$$\n",
    "    f1 = 2 \\cdot \\dfrac{(precision \\cdot recall)}{(precision + recall)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738accad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.recall_score(y_pred,y_test)) # recall = tp / (tp + fn) \n",
    "print(metrics.precision_score(y_pred,y_test)) # precision =  tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11bcc9",
   "metadata": {},
   "source": [
    "Both precision and recall are 0, but why? Let's print the entries of the confusion matrix with `metrics.confusion_matrix`.\n",
    "\n",
    "\n",
    "<img src=\"img/conf_mat.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92026f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(type(conf_mat))\n",
    "print(\"confusion matrix \\n\",conf_mat,\"\\n\")\n",
    "print(\"True negative    \\t\",conf_mat[0][0],\"--> predicted as negative, and really negative :) \")\n",
    "print(\"False positive   \\t\",conf_mat[0][1],\" --> predicted as positive, but negative :( \")\n",
    "print(\"False negative  \\t\",conf_mat[1][0],\" --> predicted as negative, but positive :( \")\n",
    "print(\"True positive   \\t\",conf_mat[1][1],\" --> predicted as positive, and really positive :) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee63dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the predictions:\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd54c5a6",
   "metadata": {},
   "source": [
    "The classifier predicts only one classe (the class zero), that's really bad! Any idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3397f4",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "1. Print the number of instances of class 0 and 1. Hint: you can use `np.unique` by setting a proper input parameter, have a look at the documentation.\n",
    "2. Print the ratio of positive samples over the total samples -> the skewness.\n",
    "3. Print the ratio of negative samples over the total samples.\n",
    "4. Is the dataset balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a8148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "print(np.unique(y_train, return_counts=True))\n",
    "# the dataset is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "neg = np.unique(y_train, return_counts=True)[1][0]\n",
    "pos = np.unique(y_train, return_counts=True)[1][1]\n",
    "\n",
    "print(f\"Number of pos instances\\t{pos}\")\n",
    "print(f\"Number of neg instances\\t{neg}\")\n",
    "print(f\"Positive ratio\\t{pos/(pos+neg)}\")\n",
    "print(f\"Negative ratio\\t{neg/(pos+neg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d065d830",
   "metadata": {},
   "source": [
    "It is a good practice to compute the positive and negative ratio before training a ML to understand the imbalance of the data.\n",
    "\n",
    "In the next exercise we address the imbalance by oversampling the minority class: giving more weights to the minority class corresponds to duplicate the samples of the minority class to reach the same number of the majority class.\n",
    "\n",
    "**Exercise 3**\n",
    "1. Train a SVM by including the class weights (see the doc, use both the `class_weight` option);\n",
    "2. The `class_weight` option takes a dict as `{0: weight_class_0, 1: weight_class_1}`, we can give weight 1 to the most popular class and a bigger weight to the minority class.\n",
    "3. Print precision, recall, f1 score, accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe62f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "print(f\"Neg weights: \\n{neg/pos}\")\n",
    "# Instantiate the model specifying the kernel\n",
    "clf = SVC(class_weight={0: 1, 1: neg/pos})\n",
    "\n",
    "# Training\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Performance\n",
    "print(f\"\\nPrecision: \\n{metrics.precision_score(y_test,y_pred)}\")\n",
    "print(f\"\\nRecall: \\n{metrics.recall_score(y_test,y_pred)}\")\n",
    "print(f\"\\nF1: \\n{metrics.f1_score(y_test,y_pred)}\")\n",
    "print(f\"\\nAccuracy: \\n{metrics.accuracy_score(y_test,y_pred)}\")\n",
    "conf_mat = metrics.confusion_matrix(y_test,y_pred)\n",
    "print(f\"\\nConfusion Matrix:\\n{conf_mat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035dbc1",
   "metadata": {},
   "source": [
    "## Plot Receiver Operating Characteristic and Precision/Recall curves\n",
    "\n",
    "Sklearn provides you utilities for computing the ROC and the PrecRec curve: the `roc_curve` and `precision_recall_curve` functions in `sklearn.metrics`. In addition, the `auc` function returns the area under each curve.\n",
    "\n",
    "**Exercise 4**\n",
    "Plot the ROC curve for each class of the Iris dataset. Understand and complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "# Import the IRIS dataset by using the datasets.load_iris() function and inspect its attributes\n",
    "# YOUR CODE HERE\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(10)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets: test size 50%, random_state=0\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel=\"linear\", probability=True, random_state=random_state))\n",
    "\n",
    "# Train the classifier and compute the scores of the test set (look at the SVM methods)\n",
    "# YOUR CODE HERE\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "# Understand what the roc_curve() does and takes as input\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "th = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    # YOUR CODE HERE\n",
    "    fpr[i], tpr[i], th[i] = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2b0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for each class with the AUC in the legend. Plot also the line from (0, 0) to (1, 1)\n",
    "# YOUR CODE HERE\n",
    "plt.figure(figsize=(7, 7))\n",
    "lw = 2\n",
    "\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    plt.plot(fpr[class_id], tpr[class_id], lw=lw, c=colors[class_id], label=f\"ROC curve for {class_name} (AUC = {roc_auc[class_id]:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d8b41-0736-4452-8452-8fea7da091f3",
   "metadata": {},
   "source": [
    "This task can be performed with the `RocCurveDisplay` function in `sklearn.metrics`. The boolean parameter `plot_chance_level` (new from version 1.3.) determines whether to plot the baseline level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fafd5-8a2e-4dcb-8d46-7fc5319138e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_test[:, class_id],\n",
    "        y_score[:, class_id],\n",
    "        name=f\"ROC curve for {class_name}\",\n",
    "        color=colors[class_id],\n",
    "        ax=ax)\n",
    "    \n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Receiver Operating Characteristic\\nto One-vs-Rest multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a448f26",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Repeat the same exercise by plotting the Precision Recall curve (search for the `precision_recall_curve` SKlearn function) with the baseline: P/(P + N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53fdf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Binarize the output\n",
    "y = label_binarize(y, classes=[0, 1, 2])\n",
    "n_classes = y.shape[1]\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# shuffle and split training and test sets: test size 50%, random_state=0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel=\"linear\", probability=True, random_state=random_state))\n",
    "\n",
    "# Train the classifier and compute the scores of the test set (look at the SVM methods)\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "pr_auc = dict()\n",
    "th = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], th[i] = precision_recall_curve(y_test[:, i], y_score[:, i])\n",
    "    pr_auc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "lw = 2\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    plt.plot(recall[class_id], precision[class_id], c=colors[class_id], lw=lw, label=f\"Precision-recall for {class_name} (AUC = {pr_auc[class_id]:.2f})\")\n",
    "\n",
    "classes, occurences = np.unique(iris.target, return_counts=True)\n",
    "skew = occurences/np.sum(occurences)\n",
    "\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    plt.plot([0, 1], [skew[class_id], skew[class_id]], color=\"navy\", lw=lw, linestyle=\"--\", label=f'Baseline for {class_name}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision Recall curve example\")\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f1372-2b70-44a2-ae7d-8b6fa41e2028",
   "metadata": {},
   "source": [
    "This task can be performed with the `PrecisionRecallDisplay` function in `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a539cb0-8388-4538-842b-118d73982226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_title=\"Precision Recall Curve\\nto One-vs-Rest multiclass\",\n",
    "\n",
    "colors = [\"aqua\", \"darkorange\", \"cornflowerblue\"]\n",
    "for class_id, class_name in enumerate(iris.target_names):\n",
    "    precision[class_id], recall[class_id], _ = precision_recall_curve(y_test[:, class_id], y_score[:, class_id])\n",
    "    average_precision[class_id] = average_precision_score(y_test[:, class_id], y_score[:, class_id])\n",
    "    display = PrecisionRecallDisplay(\n",
    "        recall=recall[class_id],\n",
    "        precision=precision[class_id],\n",
    "        average_precision=average_precision[class_id],\n",
    "    )\n",
    "    display.plot(ax=ax, name=f\"Precision-recall for class {class_name}\", color=colors[class_id])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e78fc89",
   "metadata": {},
   "source": [
    "## Model Selection: K fold, cross validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set `X_test`, `y_test`. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.\n",
    "\n",
    "One problem with validation sets is that you \"lose\" some of the data: you only used, e.g., 3/4 of the data for the training, and used 1/4 for the validation. One option is to use K-fold cross-validation, where we split the data into chunks and perform fits, where each chunk gets a turn as the validation set. \n",
    "\n",
    "First, we need to understand how to split data into folds by using `KFold`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv(\"glass.dat\")\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data['type'].values\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "for fold_id,  (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(f\"Fold id {fold_id} TRAIN: {train_index}\")\n",
    "    print(f\"Fold id {fold_id} TEST: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Label counts: {np.unique(y_train, return_counts=True)}\")\n",
    "    print(\"--------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3089e2",
   "metadata": {},
   "source": [
    "We can notice that some y labels are not balanced in the folds, use `StratifiedKFold` to obtain folds with balanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "for fold_id,  (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold id {fold_id} TRAIN: {train_index}\")\n",
    "    print(f\"Fold id {fold_id} TEST: {test_index}\")\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(f\"Label counts: {np.unique(y_train, return_counts=True)}\")\n",
    "    print(\"--------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358db2cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "- A model is trained using $K - 1$ of the folds as training data;\n",
    "\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/grid_search_cross_validation.png\" style=\"width: 400px;\"/>\n",
    "<br><br>\n",
    "\n",
    "Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques:\n",
    "\n",
    "<br><br>\n",
    "<img src=\"img/grid_search_workflow.png\" style=\"width: 400px;\"/>\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7714a",
   "metadata": {},
   "source": [
    "**Exercise 6**\n",
    "\n",
    "Use the `google_play_store_apps_reviews_training` dataset to perform 5-fold cross-validation:\n",
    "\n",
    "- Look at Lab 3 for the data import, preprocessing and splitting into train and test. Use TfidfVectorizer without stopwords;\n",
    "- Combine a `StratifiedKFold` with with `cross_val_score` of `sklearn.model_selection` to train a Support Vector Machine with linear kernel. As classification score in the `cross_val_score` object use the f1 measure (see [here](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)) in the `scoring` parameter (see [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)). For\n",
    "- print the mean and stdev of the cross validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data = pd.read_csv('google_play_store_apps_reviews_training.csv')\n",
    "vec = TfidfVectorizer()\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# data cleaning\n",
    "data = data.drop('package_name', axis=1)\n",
    "data['review'] = data['review'].str.strip().str.lower().str.replace('[^\\w]', ' ')\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data['review']\n",
    "y = data['polarity']\n",
    "\n",
    "X = vec.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=kf, scoring='f1')\n",
    "print(scores)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9e5c0",
   "metadata": {},
   "source": [
    "**Exercise 7**\n",
    "\n",
    "You can create a Sklearn pipeline to embed several ML steps. Pipelines can be used as normal classifiers, here an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "data = pd.read_csv('google_play_store_apps_reviews_training.csv')\n",
    "\n",
    "# data cleaning\n",
    "data = data.drop('package_name', axis=1)\n",
    "data['review'] = data['review'].str.strip().str.lower().str.replace('[^\\w]', ' ')\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data['review']\n",
    "y = data['polarity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "clf_pipeline = make_pipeline(CountVectorizer(), svm.SVC(kernel='linear', C=1))\n",
    "scores = cross_val_score(clf_pipeline, X_train, y_train, cv=kf, scoring='f1')\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455b97f2",
   "metadata": {},
   "source": [
    "Implement the following 3 pipelines for the above classification and evaluate them with cross_val_score:\n",
    "- `TfidfVectorizer()` and `svm.SVC(kernel='linear', C=1)`;\n",
    "- `CountVectorizer()` and the feature `Normalizer()` step, `svm.SVC(kernel='linear', C=1)`;\n",
    "- `CountVectorizer()` and `svm.SVC(kernel='linear', C=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "\n",
    "data = pd.read_csv('google_play_store_apps_reviews_training.csv')\n",
    "\n",
    "# data cleaning\n",
    "data = data.drop('package_name', axis=1)\n",
    "data['review'] = data['review'].str.strip().str.lower().str.replace('[^\\w]', ' ')\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data['review']\n",
    "y = data['polarity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "pipe_1 = make_pipeline(TfidfVectorizer(), svm.SVC(kernel='linear', C=1))\n",
    "pipe_2 = make_pipeline(CountVectorizer(), preprocessing.Normalizer(), svm.SVC(kernel='linear', C=1))\n",
    "pipe_3 = make_pipeline(CountVectorizer(), svm.SVC(kernel='linear', C=1))\n",
    "\n",
    "pipe_list = [pipe_1, pipe_2, pipe_3]\n",
    "for pipe in pipe_list:\n",
    "    print(pipe)\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=kf, scoring='f1')\n",
    "    print(scores.mean(), scores.std())\n",
    "    print(\"--------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dfb48b",
   "metadata": {},
   "source": [
    "**Exercise 8**\n",
    "\n",
    "Repeat exercise 7 by using `GridSearchCV` to perform an exhaustive search over a set of hyperparameters for a SVM with 'rbf' kernel:\n",
    "- user `TfidfVectorizer` as vectorizer;\n",
    "- use `StratifiedKFold` with 5 folds;\n",
    "- you have to define a hyperparameters grid with a Python dict: the keys are the hyperparam name, the values the range of values for that hyperparam;\n",
    "- use `[1, 10, 100]` for C and `[0.01, 0.05, 0.1]` for gamma;\n",
    "- perform `GridSearchCV`: use the f1 for the score and try to run the fold in parallel:look at the documentation how it is used;\n",
    "- export the results in a Pandas dataframe and print it.\n",
    "- What is the hyperparameters combination leading to the best average f1? What is the value of the best average f1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a59bba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 0.05}\n",
      "0.730544411027281\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
      "0       0.093242      0.002879         0.030210        0.008084       1   \n",
      "1       0.087106      0.009190         0.026903        0.006001       1   \n",
      "2       0.085982      0.003986         0.026619        0.002381       1   \n",
      "3       0.087308      0.004251         0.022393        0.003034      10   \n",
      "4       0.086498      0.008681         0.023769        0.005037      10   \n",
      "5       0.098306      0.010285         0.021538        0.005038      10   \n",
      "6       0.098951      0.004908         0.022549        0.005285     100   \n",
      "7       0.101376      0.005831         0.017250        0.001975     100   \n",
      "8       0.091850      0.005800         0.017036        0.001346     100   \n",
      "\n",
      "  param_gamma                     params  split0_test_score  \\\n",
      "0        0.01    {'C': 1, 'gamma': 0.01}           0.000000   \n",
      "1        0.05    {'C': 1, 'gamma': 0.05}           0.000000   \n",
      "2         0.1     {'C': 1, 'gamma': 0.1}           0.000000   \n",
      "3        0.01   {'C': 10, 'gamma': 0.01}           0.040816   \n",
      "4        0.05   {'C': 10, 'gamma': 0.05}           0.595238   \n",
      "5         0.1    {'C': 10, 'gamma': 0.1}           0.597701   \n",
      "6        0.01  {'C': 100, 'gamma': 0.01}           0.597701   \n",
      "7        0.05  {'C': 100, 'gamma': 0.05}           0.629213   \n",
      "8         0.1   {'C': 100, 'gamma': 0.1}           0.613636   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0           0.000000           0.000000           0.000000           0.000000   \n",
      "1           0.000000           0.000000           0.000000           0.000000   \n",
      "2           0.040816           0.000000           0.000000           0.040816   \n",
      "3           0.117647           0.040816           0.000000           0.040816   \n",
      "4           0.790698           0.738095           0.689655           0.776471   \n",
      "5           0.786517           0.729412           0.739130           0.782609   \n",
      "6           0.764045           0.729412           0.739130           0.774194   \n",
      "7           0.769231           0.735632           0.765957           0.752688   \n",
      "8           0.727273           0.712644           0.760870           0.755556   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0         0.000000        0.000000                8  \n",
      "1         0.000000        0.000000                8  \n",
      "2         0.016327        0.019996                7  \n",
      "3         0.048019        0.038235                6  \n",
      "4         0.718031        0.070684                4  \n",
      "5         0.727074        0.068565                2  \n",
      "6         0.720896        0.063694                3  \n",
      "7         0.730544        0.052029                1  \n",
      "8         0.713996        0.053244                5  \n",
      "0.7868852459016392\n",
      "0.6185567010309279\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data = pd.read_csv('google_play_store_apps_reviews_training.csv')\n",
    "vec = TfidfVectorizer()\n",
    "\n",
    "# data cleaning\n",
    "data = data.drop('package_name', axis=1)\n",
    "data['review'] = data['review'].str.strip().str.lower().str.replace('[^\\w]', ' ')\n",
    "\n",
    "# Split into training and testing data\n",
    "X = data['review']\n",
    "y = data['polarity']\n",
    "\n",
    "X = vec.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Set up possible values of parameters with a dict (a parameter grid) to optimize over.\n",
    "# Use [1, 10, 100] for C and [0.01, 0.05, 0.1] for gamma.\n",
    "p_grid = {\"C\": [1, 10, 100], \"gamma\": [0.01, 0.05, 0.1]}\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Perform GridSearchCV: use the f1 for the score and try to run the fold in parallel.\n",
    "estimator = GridSearchCV(svc, p_grid, cv=kf, scoring='f1', n_jobs=-1)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Export the results in a Pandas dataframe and print it.\n",
    "# What is the hyperparameters combination leading to the best average f1? What is the value of the best average f1?\n",
    "res_grid_search = pd.DataFrame(estimator.cv_results_)\n",
    "print(estimator.best_params_)\n",
    "print(estimator.best_score_)\n",
    "print(res_grid_search)\n",
    "\n",
    "# Lest compare the SVM with and without parameter optimization\n",
    "# svc = svm.SVC(C=100, gamma=0.05)\n",
    "svc = svm.SVC(**estimator.best_params_)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996dd51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
